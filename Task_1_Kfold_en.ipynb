{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259ed73-0216-4d26-83ef-3022566d3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy nltk tensorflow spacy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11f832-dc5b-425f-8381-c3dadf9509bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23095f06-883e-4786-aa90-a9ed55cc3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_jsonl_to_csv(jsonl_file, csv_file):\n",
    "    csv_columns = ['id', 'text', 'label']\n",
    "    \n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line) \n",
    "                filtered_data = {key: data[key] for key in csv_columns}                \n",
    "                writer.writerow(filtered_data)\n",
    "\n",
    "def test_jsonl_to_csv(jsonl_file, csv_file):\n",
    "    csv_columns = ['id', 'text']\n",
    "    \n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e862475-45db-4185-8c5d-df95563cef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_jsonl_file = 'en_train.jsonl'\n",
    "en_train_csv_file = 'en_train.csv'\n",
    "train_dev_jsonl_to_csv(en_train_jsonl_file, en_train_csv_file)\n",
    "print(f\"Data successfully written to {en_train_csv_file}\")\n",
    "\n",
    "en_dev_jsonl_file = 'en_dev.jsonl'\n",
    "en_dev_csv_file = 'en_dev.csv'\n",
    "train_dev_jsonl_to_csv(en_dev_jsonl_file, en_dev_csv_file)\n",
    "print(f\"Data successfully written to {en_dev_csv_file}\")\n",
    "\n",
    "en_test_jsonl_file = 'en_devtest_text_id_only.jsonl'\n",
    "en_test_csv_file = 'en_devtest.csv'\n",
    "test_jsonl_to_csv(en_test_jsonl_file, en_test_csv_file)\n",
    "print(f\"Data successfully written to {en_test_csv_file}\")\n",
    "\n",
    "# add datasets\n",
    "train = pd.read_csv(\"en_train.csv\")\n",
    "dev = pd.read_csv(\"en_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207b421-b7e5-41c8-9a50-ad34389d41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train['label'].astype(float)\n",
    "dev['label'] = dev['label'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28658953-1c85-4487-913d-1ec171380490",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183c0a3-d9e8-4aae-9484-7862caf731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2d5a9-7a79-4f4b-a0f1-3dfede0d343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def _init_(self, data):\n",
    "        self.data = data\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean the input text by removing URLs, mentions, hashtags, numbers, punctuations, etc.\"\"\"\n",
    "        text = re.sub(r\"@\\S+\", \"\", text)  \n",
    "        text = re.sub(r\"http[s]?\\://\\S+\", \"\", text) \n",
    "        text = re.sub(r\"#\\S+\", \"\", text)  \n",
    "        text = re.sub(r\"[0-9]\", \"\", text) \n",
    "        text = re.sub(r\"[\\[\\]()]\", \"\", text)\n",
    "        text = re.sub(r\"\\n\", \"\", text)  \n",
    "        text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "        text = re.sub(r'[^\\w\\s]', '', text) \n",
    "        text = text.lower()  \n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  \n",
    "        return text if text else \"no text\"\n",
    "    \n",
    "    def lemmatize_sentence(self, sentence):\n",
    "        \"\"\"Apply lemmatization to a sentence using SpaCy.\"\"\"\n",
    "        doc = self.nlp(sentence)\n",
    "        lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "        return lemmatized_sentence\n",
    "    \n",
    "    def stem_sentence(self, sentence):\n",
    "        \"\"\"Apply stemming to a sentence using NLTK's Snowball Stemmer.\"\"\"\n",
    "        words = sentence.split()\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        stemmed_sentence = \" \".join(stemmed_words)\n",
    "        return stemmed_sentence\n",
    "    \n",
    "    def pos_tagging(self, sentence):\n",
    "        \"\"\"Perform POS tagging on a sentence using SpaCy.\"\"\"\n",
    "        doc = self.nlp(sentence)\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        return \" \".join(pos_tags)\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Process the entire DataFrame, applying cleaning, lemmatization, stemming, and POS tagging.\"\"\"\n",
    "        # Clean the text\n",
    "        self.data['clean_text'] = self.data['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Lemmatize the text\n",
    "        lemmatized_text = []\n",
    "        for sentence in tqdm(self.data['clean_text'], desc='Lemmatizing'):\n",
    "            lemmatized_sentence = self.lemmatize_sentence(sentence)\n",
    "            lemmatized_text.append(lemmatized_sentence)\n",
    "        self.data['lemmatized_text'] = lemmatized_text\n",
    "        \n",
    "        # Stem the text\n",
    "        stemmed_text = []\n",
    "        for sentence in tqdm(self.data['clean_text'], desc='Stemming'):\n",
    "            stemmed_sentence = self.stem_sentence(sentence)\n",
    "            stemmed_text.append(stemmed_sentence)\n",
    "        self.data['stemmed_text'] = stemmed_text\n",
    "        \n",
    "        # POS Tagging\n",
    "        pos_tags = []\n",
    "        for sentence in tqdm(self.data['clean_text'], desc='POS tagging'):\n",
    "            pos_sentence = self.pos_tagging(sentence)\n",
    "            pos_tags.append(pos_sentence)\n",
    "        self.data['pos'] = pos_tags\n",
    "\n",
    "        self.data['combined_text'] = data['clean_text'] + ' ' + data['lemmatized_text'] + ' ' + data['stemmed_text'] + ' ' + data['pos']\n",
    "    \n",
    "    def get_processed_data(self):\n",
    "        \"\"\"Return the processed DataFrame with clean_text, lemmatized_text, stemmed_text, and pos columns.\"\"\"\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3ba4e-7151-43ca-b643-84c6bb33c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor = TextPreprocessor(train)\n",
    "dev_preprocessor = TextPreprocessor(dev)\n",
    "\n",
    "train_preprocessor.process_data()\n",
    "dev_preprocessor.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433e8e8-4f1b-42a4-9f30-fbcbd9bb0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_preprocessor.get_processed_data()\n",
    "dev_data = dev_preprocessor.get_processed_data()\n",
    "train_data['combined_text'] = train_data['clean_text'] + ' ' + train_data['lemmatized_text'] + ' ' + train_data['stemmed_text'] + ' ' + train_data['pos']\n",
    "dev_data['combined_text'] = dev_data['clean_text'] + ' ' + dev_data['lemmatized_text'] + ' ' + dev_data['stemmed_text'] + ' ' + dev_data['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678fea2-0ae5-49a7-a756-3d439518f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d274f7a-f8f3-4860-b539-dda05f4432a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735938fa-1449-4319-9030-b5a95f555785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization using all combined text features\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "train_x_tfidf = tfidf.fit_transform(train_x).toarray()\n",
    "valid_x_tfidf = tfidf.transform(valid_x).toarray()\n",
    "test_x_tfidf = tfidf.transform(test_x).toarray()\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_y = np.array(train_y)\n",
    "valid_y = np.array(valid_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x_tfidf, train_y)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_x_tfidf, valid_y)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x_tfidf, test_y)).batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c3670-e10d-4083-a668-369d5f91dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Build and compile the CNN model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv1D(128, 3, activation='relu', input_shape=(train_x_tfidf_reshaped.shape[1], 1)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(64, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kf.split(train_x_tfidf_reshaped):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf_reshaped[train_index], train_x_tfidf_reshaped[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Build and train the model\n",
    "    model_cnn_kfold = build_model()\n",
    "    model_cnn_kfold.fit(X_train, y_train, validation_data=(valid_x_tfidf_reshaped, valid_y), epochs=5, verbose=1)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = (model_cnn_kfold.predict(X_val) > 0.5).astype(\"int32\").flatten()\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = (model_cnn_kfold.predict(test_x_tfidf_reshaped) > 0.5).astype(\"int32\").flatten()\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall validation metrics\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall test metrics\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "model_cnn_kfold.save('model_CNN_Kfold.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3e64c-dde4-433d-a3e7-4a38dfa61f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    \"\"\"Build and compile the LSTM-like dense model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(train_x_tfidf.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_lstm_model()\n",
    "    model.fit(X_train, y_train, validation_data=(valid_x_tfidf, valid_y), epochs=5, verbose=1)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = (model.predict(X_val) > 0.5).astype(\"int32\").flatten()\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = (model.predict(test_x_tfidf) > 0.5).astype(\"int32\").flatten()\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b3443-e6bf-4269-bbd7-c3e372437292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "def build_rnn_model():\n",
    "    \"\"\"Build and compile the RNN-like dense model.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(train_x_tfidf.shape[1],)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_rnn_model()\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=5,\n",
    "        validation_data=(valid_x_tfidf, valid_y),\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_prob = model.predict(X_val)\n",
    "    val_pred_y = (val_pred_prob > 0.5).astype(int).flatten()\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_prob = model.predict(test_x_tfidf)\n",
    "    test_pred_y = (test_pred_prob > 0.5).astype(int).flatten()\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50bdb1-21c6-477c-9dc1-d19771cf5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the SVM model\n",
    "    svm_model = svm.SVC(kernel='linear', probability=True)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = svm_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = svm_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "joblib.dump(svm_model, 'svm_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc3a35-4b5c-4876-a830-84a4c04811d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the KNN model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = knn_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = knn_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "joblib.dump(knn_model, 'knn_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade29dcc-8c4d-4f19-8388-443afe4adb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the Decision Tree model\n",
    "    dt_model = DecisionTreeClassifier(random_state=42)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = dt_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = dt_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "joblib.dump(dt_model, 'dt_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f54241-d89b-4659-9c51-81c7791530f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the AdaBoost model\n",
    "    dt_stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "    ada_model = AdaBoostClassifier(estimator=dt_stump, n_estimators=100, algorithm='SAMME', random_state=42)\n",
    "    ada_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = ada_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = ada_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "joblib.dump(ada_model, 'ada_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45af18-254e-490b-b580-eb69698f0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the Bagging model\n",
    "    base_estimator = DecisionTreeClassifier()\n",
    "    bagging_model = BaggingClassifier(estimator=base_estimator, n_estimators=100, random_state=42)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = bagging_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = bagging_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "joblib.dump(bagging_model, 'bagging_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a058ec9-bdf5-4b30-88db-15e6b348c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the Gradient Boosting model\n",
    "    gradient_boosting_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    gradient_boosting_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = gradient_boosting_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = gradient_boosting_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(gradient_boosting_model, 'gradient_boosting_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48adb45-e406-4c95-b420-f35bcff492da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = rf_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = rf_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(rf_model, 'random_forest_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0745b-7570-4827-85b6-650aeec4e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the Extra Trees model\n",
    "    extra_trees_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "    extra_trees_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = extra_trees_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = extra_trees_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(extra_trees_model, 'extra_trees_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaac2c-d947-4cf3-b2c5-5187a6c3e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the Logistic Regression model\n",
    "    log_reg = LogisticRegression(max_iter=1000)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = log_reg.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = log_reg.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(log_reg, 'logistic_regression_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11da5e-1157-4caa-b222-717d1672c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store overall predictions and true labels for validation and test sets\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # Train the MLP model\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    val_pred_y = mlp_model.predict(X_val)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    # Test predictions\n",
    "    test_pred_y = mlp_model.predict(test_x_tfidf)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Compute overall metrics for validation\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "# Compute overall metrics for test\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred)\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "# Display overall validation metrics\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "# Display overall test metrics\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "# Plot confusion matrices\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(mlp_model, 'mlp_model_Kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fdfd0-688a-438f-8a58-6c14e85b1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, matthews_corrcoef, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "overall_valid_true, overall_valid_pred = [], []\n",
    "overall_test_true, overall_test_pred = [], []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kf.split(train_x_tfidf):\n",
    "    print(f\"\\n\\n--- Fold {fold} ---\")\n",
    "\n",
    "    X_train, X_val = train_x_tfidf[train_index], train_x_tfidf[val_index]\n",
    "    y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=10, max_iter=10, random_state=42)\n",
    "    X_train_lda = lda_model.fit_transform(X_train)\n",
    "    X_val_lda = lda_model.transform(X_val)\n",
    "    test_x_lda = lda_model.transform(test_x_tfidf)\n",
    "\n",
    "    val_pred_y = np.argmax(X_val_lda, axis=1)\n",
    "    overall_valid_true.extend(y_val)\n",
    "    overall_valid_pred.extend(val_pred_y)\n",
    "\n",
    "    test_pred_y = np.argmax(test_x_lda, axis=1)\n",
    "    overall_test_true.extend(test_y)\n",
    "    overall_test_pred.extend(test_pred_y)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "overall_valid_accuracy = accuracy_score(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_f1 = f1_score(overall_valid_true, overall_valid_pred, average='weighted')\n",
    "overall_valid_mcc = matthews_corrcoef(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_cmd = confusion_matrix(overall_valid_true, overall_valid_pred)\n",
    "overall_valid_report = classification_report(overall_valid_true, overall_valid_pred)\n",
    "\n",
    "overall_test_accuracy = accuracy_score(overall_test_true, overall_test_pred)\n",
    "overall_test_f1 = f1_score(overall_test_true, overall_test_pred, average='weighted')\n",
    "overall_test_mcc = matthews_corrcoef(overall_test_true, overall_test_pred)\n",
    "overall_test_cmd = confusion_matrix(overall_test_true, overall_test_pred)\n",
    "overall_test_report = classification_report(overall_test_true, overall_test_pred)\n",
    "\n",
    "print(\"\\n\\n--- Overall Validation Metrics ---\")\n",
    "print(f\"Accuracy: {overall_valid_accuracy}\")\n",
    "print(f\"F1 Score: {overall_valid_f1}\")\n",
    "print(f\"MCC: {overall_valid_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_valid_cmd)\n",
    "print(\"Classification Report:\\n\", overall_valid_report)\n",
    "\n",
    "print(\"\\n\\n--- Overall Test Metrics ---\")\n",
    "print(f\"Accuracy: {overall_test_accuracy}\")\n",
    "print(f\"F1 Score: {overall_test_f1}\")\n",
    "print(f\"MCC: {overall_test_mcc}\")\n",
    "print(\"Confusion Matrix:\\n\", overall_test_cmd)\n",
    "print(\"Classification Report:\\n\", overall_test_report)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_valid_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix=overall_test_cmd, display_labels=['Human', 'Generated']).plot()\n",
    "\n",
    "joblib.dump(lda_model, 'lda_model_Kfold.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
